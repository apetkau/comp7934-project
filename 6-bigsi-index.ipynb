{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: BIGSI Indexing\n",
    "\n",
    "Now let's run BIGSI to index on our data.\n",
    "\n",
    "First, let's setup some directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_data_dir=data-downsampled\n",
    "data_dir=kmer-counts-jellyfish\n",
    "bigsi_dir=bigsi\n",
    "kmer_size=\"17\"\n",
    "\n",
    "threads=1\n",
    "\n",
    "PROJECT_DIR=`git rev-parse --show-toplevel`\n",
    "cd $PROJECT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code given below assumes you have the following [conda](https://docs.conda.io/en/latest/) environments setup to install [mccortex](https://github.com/mcveanlab/mccortex) and [bigsi](https://github.com/Phelimb/BIGSI). This can be done with.\n",
    "\n",
    "```bash\n",
    "conda create --name bigsi_mccortex mccortex\n",
    "conda create --name bigsi_mccortex bigsi\n",
    "```\n",
    "\n",
    "Let's verify these commands exist (and verify versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda run --name bigsi_mccortex mccortex 31 build --help 2>&1 | grep 'mccortex=v'\n",
    "conda run --name bigsi_mccortex bigsi bloom --help 2>&1 | grep 'bigsi-v'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now let's look at figuring out the optimal Bloom filter sizes and hash functions for BIGSI and HowDeSBT.\n",
    "\n",
    "## Bloom filter sizes\n",
    "\n",
    "First, let's pull out the maximum number of unique (canonical) kmers (for kmer size 15) from each dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(echo -e \"data_type\\tkmer_size\\tmax_kmers\"\n",
    "for data_type in microbial metagenomics human\n",
    "do\n",
    "    # Get max kmer counts from kmers counted by jellyfish (for kmer size 15)\n",
    "    # tail -n+2 removes header line (first line) from data\n",
    "    max_kmer_count=`tail -n+2 ${data_type}/${data_dir}/${kmer_size}/kmer-counts.tsv | sort -k2,2n | cut -f 2 | tail -n 1`\n",
    "    echo -e \"${data_type}\\t${kmer_size}\\t${max_kmer_count}\"\n",
    "done) | sort -k3,3n | column -s$'\\t' -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also pull out the estimated union of all k-mers across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(echo -e \"data_type\\tkmer_size\\tunion_kmers\"\n",
    "for data_type in microbial metagenomics human\n",
    "do\n",
    "    count=`sed -e 's/Estimated number of unique exact matches: //' ${data_type}/${fastq_data_dir}/total-unique-kmers-${kmer_size}.txt | \\\n",
    "    awk '{print int($1+0.5)}'`\n",
    "    \n",
    "    echo -e \"${data_type}\\t${kmer_size}\\t${count}\"\n",
    "done) | sort -k3,3n | column -s$'\\t' -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the union of unique kmers we'll use to set Bloom filter sizes. We'll set these as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For kmer size 9\n",
    "#microbial_bits=140000\n",
    "#human_bits=140000\n",
    "#metagenomics_bits=140000\n",
    "# For kmer size 11\n",
    "#microbial_bits=2100000\n",
    "#human_bits=2100000\n",
    "#metagenomics_bits=2100000\n",
    "# For kmer size 13\n",
    "#microbial_bits=24000000\n",
    "#human_bits=24000000\n",
    "#metagenomics_bits=33000000\n",
    "# For kmer size 15\n",
    "#microbial_bits=61000000\n",
    "#human_bits=72000000\n",
    "#metagenomics_bits=210000000\n",
    "# For kmer size 17\n",
    "microbial_bits=74000000\n",
    "human_bits=86000000\n",
    "metagenomics_bits=340000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BIGSI config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bigsi_config() {\n",
    "    kmer_size_config=$1\n",
    "    bits=$2\n",
    "    hashes=$3\n",
    "    output_dir_config=$4\n",
    "\n",
    "echo \"## Example config using berkeleyDB\n",
    "h: ${hashes}\n",
    "k: ${kmer_size_config}\n",
    "m: ${bits}\n",
    "storage-engine: berkeleydb\n",
    "storage-config:\n",
    "  filename: ${output_dir_config}/kmer${kmer_size_config}-bits${bits}-hashes${hashes}-bigsi.db\n",
    "  flag: \"c\" ## Change to 'r' for read-only access\n",
    "\" > ${output_dir_config}/berkelydb.yaml\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type=microbial\n",
    "output_dir=${data_type}/${bigsi_dir}/${kmer_size}\n",
    "mkdir -p ${output_dir}\n",
    "create_bigsi_config \"${kmer_size}\" \"${microbial_bits}\" \"1\" \"${output_dir}\"\n",
    "ls -l ${output_dir}/berkelydb.yaml\n",
    "\n",
    "data_type=metagenomics\n",
    "output_dir=${data_type}/${bigsi_dir}/${kmer_size}\n",
    "mkdir -p ${output_dir}\n",
    "create_bigsi_config \"${kmer_size}\" \"${metagenomics_bits}\" \"1\" \"${output_dir}\"\n",
    "ls -l ${output_dir}/berkelydb.yaml\n",
    "\n",
    "data_type=human\n",
    "output_dir=${data_type}/${bigsi_dir}/${kmer_size}\n",
    "mkdir -p ${output_dir}\n",
    "create_bigsi_config \"${kmer_size}\" \"${human_bits}\" \"1\" \"${output_dir}\"\n",
    "ls -l ${output_dir}/berkelydb.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this step is complete, we now need to define the bash function for constructing the BIGSI Bloom filters. BIGSI requires a cortex file as input, which can be generated from the kmer list in the previous `jellyfish` step by running first through `mccortex`.\n",
    "\n",
    "We will first do this, then generate the BIGSI Bloom filters.\n",
    "\n",
    "## BIGSI Bloom filter bash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_bigsi_bloom() {\n",
    "    type_dir=$1\n",
    "    output_dir=$2\n",
    "    nkmers=$3\n",
    "    mem=$4\n",
    "    jobs=$5\n",
    "    \n",
    "    input_dir=${type_dir}/${data_dir}/${kmer_size}\n",
    "    \n",
    "    export BIGSI_CONFIG=${output_dir}/berkelydb.yaml\n",
    "            \n",
    "    before=`date +%s`\n",
    "    \n",
    "    rm ${output_dir}/mccortex.*\n",
    "    rm -rf ${output_dir}/bigsi.*\n",
    "    rm ${output_dir}/*bigsi.db\n",
    "\n",
    "    commands_file=`mktemp`\n",
    "    \n",
    "    for file in ${input_dir}/*.kmer.gz\n",
    "    do\n",
    "        accession=`basename ${file} .kmer.gz`\n",
    "\n",
    "        mccortex_out=${output_dir}/mccortex.${accession}.ctx\n",
    "        mccortex_log=${output_dir}/mccortex.count.${accession}.log\n",
    "        \n",
    "        bigsi_out=${output_dir}/bigsi.${accession}.bloom\n",
    "        bigsi_log=${output_dir}/bigsi.${accession}.bloom.log\n",
    "\n",
    "        command=\"/usr/bin/time -v mccortex ${kmer_size} build --nkmers ${nkmers} --threads ${threads} --kmer ${kmer_size} \\\n",
    "            --mem ${mem} --sample ${accession} --seq ${file} ${mccortex_out} 2> ${mccortex_log}.err 1> ${mccortex_log} && \\\n",
    "            /usr/bin/time -v bigsi bloom ${mccortex_out} ${bigsi_out} 2> ${bigsi_log}.err 1> ${bigsi_log}\"\n",
    "        echo ${command} >> ${commands_file}\n",
    "    done\n",
    "    \n",
    "    echo \"Will run commands (mccortex and bigsi bloom) from [${commands_file}] like:\"\n",
    "    head -n 1 ${commands_file}\n",
    "    command=\"parallel -j ${jobs} -a ${commands_file}\"\n",
    "    echo -e \"\\n${command}\"\n",
    "    conda run --name bigsi_mccortex ${command}\n",
    "    \n",
    "    echo -e \"\\nNow, let's merge all these files together into a single BIGSI database.\"\n",
    "    bigsi_merge_log=${output_dir}/bigsi.build.log\n",
    "    files=`echo -n ${output_dir}/*.bloom`\n",
    "    samples=`for file in ${files}; do echo -n \"-s \"; basename ${file} .bloom | sed -e 's/^bigsi\\.//'; done`\n",
    "    command=\"/usr/bin/time -v bigsi build ${files} ${samples} 2> ${bigsi_merge_log}.err 1> ${bigsi_merge_log}\"\n",
    "    echo ${command}\n",
    "    conda run --name bigsi_mccortex ${command}\n",
    "    \n",
    "    after=`date +%s`\n",
    "    minutes=`echo \"(${after}-${before})/60\" | bc -l`\n",
    "    printf \"Done. Took %0.2f minutes.\" ${minutes}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our mccortex code defined. Let's run it on a dataset.\n",
    "\n",
    "## Microbial bigsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_type=\"microbial\"\n",
    "run_bigsi_bloom \"${input_dir_type}\" \"${input_dir_type}/${bigsi_dir}/${kmer_size}\" \"${microbial_bits}\" \"3G\" \"24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. It's all finished. Let's look at some of the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du -sh ${input_dir_type}/${bigsi_dir}/${kmer_size}/*.bloom | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These contain the individual BIGSI Bloom filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du -sh ${input_dir_type}/${bigsi_dir}/${kmer_size}/*.ctx | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the intermediate cortex graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh ${input_dir_type}/${bigsi_dir}/${kmer_size}/*bigsi.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final BIGSI database.\n",
    "\n",
    "Let's now measure the sizes of the intermediate files/database size on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du -mc ${input_dir_type}/${bigsi_dir}/${kmer_size}/{*.ctx,*.bloom} | \n",
    "    grep 'total' | \n",
    "    sed -e 's/\\ttotal$/ total intermediate (MB)/' | \n",
    "    tee ${input_dir_type}/${bigsi_dir}/${kmer_size}/bigsi-total-disk.txt\n",
    "    \n",
    "du -mc ${input_dir_type}/${bigsi_dir}/${kmer_size}/*bigsi.db |\n",
    "    grep 'total' | \n",
    "    sed -e 's/\\ttotal$/ total database (MB)/' |\n",
    "    tee -a ${input_dir_type}/${bigsi_dir}/${kmer_size}/bigsi-total-disk.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metagenomics bigsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_type=\"metagenomics\"\n",
    "run_bigsi_bloom \"${input_dir_type}\" \"${input_dir_type}/${bigsi_dir}/${kmer_size}\" \"${metagenomics_bits}\" \"10G\" \"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du -mc ${input_dir_type}/${bigsi_dir}/${kmer_size}/{*.ctx,*.bloom} | \n",
    "    grep 'total' | \n",
    "    sed -e 's/\\ttotal$/ total intermediate (MB)/' | \n",
    "    tee ${input_dir_type}/${bigsi_dir}/${kmer_size}/bigsi-total-disk.txt\n",
    "    \n",
    "du -mc ${input_dir_type}/${bigsi_dir}/${kmer_size}/*bigsi.db |\n",
    "    grep 'total' | \n",
    "    sed -e 's/\\ttotal$/ total database (MB)/' |\n",
    "    tee -a ${input_dir_type}/${bigsi_dir}/${kmer_size}/bigsi-total-disk.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human bigsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_type=\"human\"\n",
    "run_bigsi_bloom \"${input_dir_type}\" \"${input_dir_type}/${bigsi_dir}/${kmer_size}\" \"${human_bits}\" \"5G\" \"12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du -mc ${input_dir_type}/${bigsi_dir}/${kmer_size}/{*.ctx,*.bloom} | \n",
    "    grep 'total' | \n",
    "    sed -e 's/\\ttotal$/ total intermediate (MB)/' | \n",
    "    tee ${input_dir_type}/${bigsi_dir}/${kmer_size}/bigsi-total-disk.txt\n",
    "    \n",
    "du -mc ${input_dir_type}/${bigsi_dir}/${kmer_size}/*bigsi.db |\n",
    "    grep 'total' | \n",
    "    sed -e 's/\\ttotal$/ total database (MB)/' |\n",
    "    tee -a ${input_dir_type}/${bigsi_dir}/${kmer_size}/bigsi-total-disk.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
